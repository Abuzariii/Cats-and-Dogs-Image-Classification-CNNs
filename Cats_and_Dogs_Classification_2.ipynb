{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cats_and_Dogs_pt2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cybftdq2Ql9R"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook I will try to beat our accuracy score of 76% from the previous notebook."
      ],
      "metadata": {
        "id": "oW-oaZalQU3R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ted0NyVCgIsh"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Importing and Preprocessing"
      ],
      "metadata": {
        "id": "cybftdq2Ql9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTswJoDcgOCP",
        "outputId": "b4615565-a62b-4d58-b2e3-66efcb4ac804"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip gdrive/MyDrive/archive.zip"
      ],
      "metadata": {
        "id": "EqM0QQlPQguK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "num_cat_images_train = len(os.listdir(\"training_set/training_set/cats\"))\n",
        "num_dog_images_train = len(os.listdir(\"training_set/training_set/dogs\"))\n",
        "num_cat_images_test = len(os.listdir(\"test_set/test_set/cats\"))\n",
        "num_dog_images_test = len(os.listdir(\"test_set/test_set/dogs\"))"
      ],
      "metadata": {
        "id": "5jS10BlxgYao"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training dataset has {num_cat_images_train} cat images and {num_dog_images_train} dog images.\")\n",
        "print(f\"Test dataset has {num_cat_images_test} cat images and {num_dog_images_test} dog images.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Und2Fa1wgaHn",
        "outputId": "ed25354b-a8b3-4f78-e19a-46d35acfcc3d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset has 4001 cat images and 4006 dog images.\n",
            "Test dataset has 1012 cat images and 1013 dog images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the class names (programmatically, this is much more helpful with a longer list of classes)\n",
        "\n",
        "class_names = [ item for item in os.listdir('test_set/test_set') if os.path.isdir(os.path.join(\"test_set/test_set\", item)) ]\n",
        "print(class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FerqWj6vgcoW",
        "outputId": "e44633dc-8078-4d20-8cef-cc57add25a49"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cats', 'dogs']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Data Normalization\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Setup the train and test directories\n",
        "train_dir = \"training_set/training_set/\"\n",
        "test_dir = \"test_set/test_set/\"\n",
        "\n",
        "# Import data from directories and turn it into batches\n",
        "train_data = train_datagen.flow_from_directory(train_dir,\n",
        "                                               batch_size=32, # number of images to process at a time \n",
        "                                               target_size=(224, 224), # convert all images to be 224 x 224\n",
        "                                               class_mode=\"binary\", # type of problem we're working on\n",
        "                                               seed=42)\n",
        "\n",
        "test_data = valid_datagen.flow_from_directory(test_dir,\n",
        "                                              batch_size=32,\n",
        "                                              target_size=(224, 224),\n",
        "                                              class_mode=\"binary\",\n",
        "                                              seed=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS2xeEmbhNSF",
        "outputId": "975c1487-32f9-46cf-a279-b1378351c8a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8005 images belonging to 2 classes.\n",
            "Found 2023 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    This will flip randomly rotate our images between 0 and 120 degrees, zoom\n",
        "    them a little bit, shift them across their width and height and will also\n",
        "    horizontally flip them randomly.\n",
        "\"\"\"\n",
        "\n",
        "train_datagen_augmented = ImageDataGenerator(rescale = 1/255,\n",
        "                                          rotation_range=20,\n",
        "                                          shear_range=0.2,\n",
        "                                          zoom_range=0.2,\n",
        "                                          width_shift_range=0.2,\n",
        "                                          height_shift_range=0.2,\n",
        "                                          horizontal_flip=True)\n",
        "\n",
        "# Create ImageDataGenerator training instance without data augmentation\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.) \n",
        "\n",
        "# Create ImageDataGenerator test instance without data augmentation\n",
        "test_datagen = ImageDataGenerator(rescale=1/255.)"
      ],
      "metadata": {
        "id": "8mcPpGcugdzr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data and augment it from training directory\n",
        "print(\"Augmented training images:\")\n",
        "train_data_augmented = train_datagen_augmented.flow_from_directory(train_dir,\n",
        "                                                                   target_size=(224, 224),\n",
        "                                                                   batch_size=32,\n",
        "                                                                   class_mode='binary',\n",
        "                                                                   shuffle=False)\n",
        "\n",
        "# Create non-augmented data batches\n",
        "print(\"Non-augmented training images:\")\n",
        "train_data = train_datagen.flow_from_directory(train_dir,\n",
        "                                               target_size=(224, 224),\n",
        "                                               batch_size=32,\n",
        "                                               class_mode='binary',\n",
        "                                               shuffle=False) # Don't shuffle for demonstration purposes\n",
        "\n",
        "print(\"Unchanged test images:\")\n",
        "test_data = test_datagen.flow_from_directory(test_dir,\n",
        "                                             target_size=(224, 224),\n",
        "                                             batch_size=32,\n",
        "                                             class_mode='binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppg5fKnJg5L7",
        "outputId": "802a402a-1709-4425-efcd-7c16f0893559"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented training images:\n",
            "Found 8005 images belonging to 2 classes.\n",
            "Non-augmented training images:\n",
            "Found 8005 images belonging to 2 classes.\n",
            "Unchanged test images:\n",
            "Found 2023 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data batch samples\n",
        "images, labels = train_data.next()\n",
        "augmented_images, augmented_labels = train_data_augmented.next()"
      ],
      "metadata": {
        "id": "6mCAOK-dhB5B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data and augment it from directories\n",
        "train_data_augmented_shuffled = train_datagen_augmented.flow_from_directory(train_dir,\n",
        "                                                                            target_size=(224, 224),\n",
        "                                                                            batch_size=32,\n",
        "                                                                            class_mode='binary',\n",
        "                                                                            shuffle=True) # Shuffle data (default)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz_sRCTfihT6",
        "outputId": "04b836aa-ef3c-49fc-d568-798c3606fbdd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8005 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1 : CNN"
      ],
      "metadata": {
        "id": "2agtcN-xQrmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Activation\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Create the model\n",
        "model_1 = Sequential([\n",
        "  Conv2D(32, 3, activation='relu', input_shape=(224, 224, 3), kernel_initializer='he_uniform'),\n",
        "  MaxPool2D(pool_size=2),\n",
        "  Conv2D(64, 3, activation='relu', kernel_initializer='he_uniform'),\n",
        "  MaxPool2D(),\n",
        "  Conv2D(128, 3, activation='relu', kernel_initializer='he_uniform'),\n",
        "  MaxPool2D(),\n",
        "  Flatten(),\n",
        "  Dense(128, activation=\"relu\", kernel_initializer='he_uniform'),\n",
        "  Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_1.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history_1 = model_1.fit(train_data_augmented_shuffled,\n",
        "                        epochs=10,\n",
        "                        steps_per_epoch=len(train_data_augmented),\n",
        "                        validation_data=test_data,\n",
        "                        validation_steps=0.5*len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWRIbhQOhXtK",
        "outputId": "87b8492d-4e40-4d5f-ef5a-36d45865010a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "251/251 [==============================] - 118s 419ms/step - loss: 0.7022 - accuracy: 0.5462 - val_loss: 0.6706 - val_accuracy: 0.5947\n",
            "Epoch 2/10\n",
            "251/251 [==============================] - 104s 413ms/step - loss: 0.6691 - accuracy: 0.5800 - val_loss: 0.6849 - val_accuracy: 0.5469\n",
            "Epoch 3/10\n",
            "251/251 [==============================] - 105s 418ms/step - loss: 0.6514 - accuracy: 0.6140 - val_loss: 0.6679 - val_accuracy: 0.5449\n",
            "Epoch 4/10\n",
            "251/251 [==============================] - 102s 407ms/step - loss: 0.6368 - accuracy: 0.6320 - val_loss: 0.6911 - val_accuracy: 0.5605\n",
            "Epoch 5/10\n",
            "251/251 [==============================] - 103s 412ms/step - loss: 0.6225 - accuracy: 0.6472 - val_loss: 0.5809 - val_accuracy: 0.6953\n",
            "Epoch 6/10\n",
            "251/251 [==============================] - 102s 406ms/step - loss: 0.6111 - accuracy: 0.6622 - val_loss: 0.5703 - val_accuracy: 0.6992\n",
            "Epoch 7/10\n",
            "251/251 [==============================] - 101s 404ms/step - loss: 0.6053 - accuracy: 0.6668 - val_loss: 0.5557 - val_accuracy: 0.7070\n",
            "Epoch 8/10\n",
            "251/251 [==============================] - 103s 409ms/step - loss: 0.5954 - accuracy: 0.6753 - val_loss: 0.6106 - val_accuracy: 0.6729\n",
            "Epoch 9/10\n",
            "251/251 [==============================] - 101s 403ms/step - loss: 0.5896 - accuracy: 0.6800 - val_loss: 0.5740 - val_accuracy: 0.6680\n",
            "Epoch 10/10\n",
            "251/251 [==============================] - 103s 409ms/step - loss: 0.5801 - accuracy: 0.6936 - val_loss: 0.6236 - val_accuracy: 0.6621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2 : 50 Epochs"
      ],
      "metadata": {
        "id": "u88hVu_DQ3Ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline model with data augmentation for the dogs vs cats dataset\n",
        "import sys\n",
        "import keras\n",
        "from matplotlib import pyplot\n",
        "# from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "# from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# define cnn model\n",
        "def define_model():\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# compile model\n",
        "\topt = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
        "\tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Classification Accuracy')\n",
        "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig(filename + '_plot.png')\n",
        "\tpyplot.close()\n",
        "\n",
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "\t# define model\n",
        "\tmodel = define_model()\n",
        "\t# create data generators\n",
        "\ttrain_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
        "\t\twidth_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "\ttest_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\t# prepare iterators\n",
        "\ttrain_it = train_datagen.flow_from_directory('training_set/training_set/',\n",
        "\t\tclass_mode='binary', batch_size=64, target_size=(200, 200))\n",
        "\ttest_it = test_datagen.flow_from_directory('test_set/test_set/',\n",
        "\t\tclass_mode='binary', batch_size=64, target_size=(200, 200))\n",
        "\t# fit model\n",
        "\thistory = model.fit(train_it, steps_per_epoch=len(train_it),\n",
        "                     validation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=1)\n",
        "\t# evaluate model\n",
        "\t_, acc = model.evaluate(test_it, steps=len(test_it), verbose=0)\n",
        "\tprint('> %.3f' % (acc * 100.0))\n",
        "\t# learning curves\n",
        "\tsummarize_diagnostics(history)\n",
        "\n",
        "# entry point, run the test harness\n",
        "run_test_harness()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sS5vwHFixU4",
        "outputId": "cc47c022-962c-4ef8-a4c0-f45363995cba"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8005 images belonging to 2 classes.\n",
            "Found 2023 images belonging to 2 classes.\n",
            "Epoch 1/50\n",
            "126/126 [==============================] - 88s 697ms/step - loss: 0.7095 - accuracy: 0.5344 - val_loss: 0.6823 - val_accuracy: 0.5709\n",
            "Epoch 2/50\n",
            "126/126 [==============================] - 87s 688ms/step - loss: 0.6809 - accuracy: 0.5639 - val_loss: 0.6703 - val_accuracy: 0.6011\n",
            "Epoch 3/50\n",
            "126/126 [==============================] - 87s 694ms/step - loss: 0.6682 - accuracy: 0.5875 - val_loss: 0.7172 - val_accuracy: 0.5166\n",
            "Epoch 4/50\n",
            "126/126 [==============================] - 86s 682ms/step - loss: 0.6619 - accuracy: 0.5913 - val_loss: 0.6392 - val_accuracy: 0.6352\n",
            "Epoch 5/50\n",
            "126/126 [==============================] - 86s 683ms/step - loss: 0.6484 - accuracy: 0.6144 - val_loss: 0.6360 - val_accuracy: 0.6357\n",
            "Epoch 6/50\n",
            "126/126 [==============================] - 87s 693ms/step - loss: 0.6338 - accuracy: 0.6411 - val_loss: 0.6415 - val_accuracy: 0.6303\n",
            "Epoch 7/50\n",
            "126/126 [==============================] - 86s 678ms/step - loss: 0.6255 - accuracy: 0.6441 - val_loss: 0.6200 - val_accuracy: 0.6545\n",
            "Epoch 8/50\n",
            "126/126 [==============================] - 86s 680ms/step - loss: 0.6198 - accuracy: 0.6452 - val_loss: 0.6307 - val_accuracy: 0.6303\n",
            "Epoch 9/50\n",
            "126/126 [==============================] - 87s 690ms/step - loss: 0.6146 - accuracy: 0.6560 - val_loss: 0.6028 - val_accuracy: 0.6738\n",
            "Epoch 10/50\n",
            "126/126 [==============================] - 85s 677ms/step - loss: 0.6044 - accuracy: 0.6685 - val_loss: 0.5918 - val_accuracy: 0.6787\n",
            "Epoch 11/50\n",
            "126/126 [==============================] - 85s 676ms/step - loss: 0.5945 - accuracy: 0.6750 - val_loss: 0.5930 - val_accuracy: 0.6787\n",
            "Epoch 12/50\n",
            "126/126 [==============================] - 87s 688ms/step - loss: 0.5840 - accuracy: 0.6886 - val_loss: 0.5794 - val_accuracy: 0.6950\n",
            "Epoch 13/50\n",
            "126/126 [==============================] - 85s 678ms/step - loss: 0.5770 - accuracy: 0.6936 - val_loss: 0.5789 - val_accuracy: 0.6901\n",
            "Epoch 14/50\n",
            "126/126 [==============================] - 87s 688ms/step - loss: 0.5768 - accuracy: 0.6958 - val_loss: 0.5675 - val_accuracy: 0.7049\n",
            "Epoch 15/50\n",
            "126/126 [==============================] - 85s 676ms/step - loss: 0.5621 - accuracy: 0.7157 - val_loss: 0.5637 - val_accuracy: 0.7029\n",
            "Epoch 16/50\n",
            "126/126 [==============================] - 85s 678ms/step - loss: 0.5585 - accuracy: 0.7104 - val_loss: 0.5382 - val_accuracy: 0.7390\n",
            "Epoch 17/50\n",
            "126/126 [==============================] - 86s 684ms/step - loss: 0.5541 - accuracy: 0.7127 - val_loss: 0.5556 - val_accuracy: 0.7276\n",
            "Epoch 18/50\n",
            "126/126 [==============================] - 85s 676ms/step - loss: 0.5537 - accuracy: 0.7107 - val_loss: 0.5320 - val_accuracy: 0.7355\n",
            "Epoch 19/50\n",
            "126/126 [==============================] - 86s 682ms/step - loss: 0.5393 - accuracy: 0.7314 - val_loss: 0.5182 - val_accuracy: 0.7459\n",
            "Epoch 20/50\n",
            "126/126 [==============================] - 85s 675ms/step - loss: 0.5366 - accuracy: 0.7275 - val_loss: 0.5142 - val_accuracy: 0.7499\n",
            "Epoch 21/50\n",
            "126/126 [==============================] - 85s 676ms/step - loss: 0.5346 - accuracy: 0.7337 - val_loss: 0.5094 - val_accuracy: 0.7573\n",
            "Epoch 22/50\n",
            "126/126 [==============================] - 86s 681ms/step - loss: 0.5221 - accuracy: 0.7362 - val_loss: 0.5106 - val_accuracy: 0.7573\n",
            "Epoch 23/50\n",
            "126/126 [==============================] - 91s 723ms/step - loss: 0.5098 - accuracy: 0.7488 - val_loss: 0.4902 - val_accuracy: 0.7716\n",
            "Epoch 24/50\n",
            "126/126 [==============================] - 85s 676ms/step - loss: 0.5129 - accuracy: 0.7438 - val_loss: 0.5014 - val_accuracy: 0.7573\n",
            "Epoch 25/50\n",
            "126/126 [==============================] - 86s 683ms/step - loss: 0.4970 - accuracy: 0.7608 - val_loss: 0.4828 - val_accuracy: 0.7682\n",
            "Epoch 26/50\n",
            "126/126 [==============================] - 85s 676ms/step - loss: 0.4953 - accuracy: 0.7595 - val_loss: 0.4830 - val_accuracy: 0.7642\n",
            "Epoch 27/50\n",
            "126/126 [==============================] - 85s 675ms/step - loss: 0.4969 - accuracy: 0.7564 - val_loss: 0.4799 - val_accuracy: 0.7637\n",
            "Epoch 28/50\n",
            "126/126 [==============================] - 86s 686ms/step - loss: 0.4858 - accuracy: 0.7630 - val_loss: 0.4762 - val_accuracy: 0.7761\n",
            "Epoch 29/50\n",
            "126/126 [==============================] - 85s 673ms/step - loss: 0.4787 - accuracy: 0.7683 - val_loss: 0.4887 - val_accuracy: 0.7662\n",
            "Epoch 30/50\n",
            "126/126 [==============================] - 85s 672ms/step - loss: 0.4778 - accuracy: 0.7716 - val_loss: 0.4697 - val_accuracy: 0.7672\n",
            "Epoch 31/50\n",
            "126/126 [==============================] - 86s 684ms/step - loss: 0.4764 - accuracy: 0.7724 - val_loss: 0.4652 - val_accuracy: 0.7860\n",
            "Epoch 32/50\n",
            "126/126 [==============================] - 85s 671ms/step - loss: 0.4668 - accuracy: 0.7800 - val_loss: 0.4713 - val_accuracy: 0.7716\n",
            "Epoch 33/50\n",
            "126/126 [==============================] - 89s 710ms/step - loss: 0.4644 - accuracy: 0.7775 - val_loss: 0.4633 - val_accuracy: 0.7766\n",
            "Epoch 34/50\n",
            "126/126 [==============================] - 85s 674ms/step - loss: 0.4661 - accuracy: 0.7784 - val_loss: 0.4566 - val_accuracy: 0.7785\n",
            "Epoch 35/50\n",
            "126/126 [==============================] - 85s 672ms/step - loss: 0.4552 - accuracy: 0.7839 - val_loss: 0.4733 - val_accuracy: 0.7751\n",
            "Epoch 36/50\n",
            "126/126 [==============================] - 86s 681ms/step - loss: 0.4534 - accuracy: 0.7860 - val_loss: 0.4707 - val_accuracy: 0.7746\n",
            "Epoch 37/50\n",
            "126/126 [==============================] - 85s 675ms/step - loss: 0.4540 - accuracy: 0.7863 - val_loss: 0.4553 - val_accuracy: 0.7825\n",
            "Epoch 38/50\n",
            "126/126 [==============================] - 86s 686ms/step - loss: 0.4503 - accuracy: 0.7890 - val_loss: 0.4341 - val_accuracy: 0.8028\n",
            "Epoch 39/50\n",
            "126/126 [==============================] - 85s 675ms/step - loss: 0.4481 - accuracy: 0.7881 - val_loss: 0.4310 - val_accuracy: 0.8018\n",
            "Epoch 40/50\n",
            "126/126 [==============================] - 85s 676ms/step - loss: 0.4358 - accuracy: 0.8004 - val_loss: 0.4466 - val_accuracy: 0.7929\n",
            "Epoch 41/50\n",
            "126/126 [==============================] - 87s 689ms/step - loss: 0.4321 - accuracy: 0.8017 - val_loss: 0.4368 - val_accuracy: 0.8043\n",
            "Epoch 42/50\n",
            "126/126 [==============================] - 86s 679ms/step - loss: 0.4274 - accuracy: 0.8034 - val_loss: 0.4245 - val_accuracy: 0.8062\n",
            "Epoch 43/50\n",
            "126/126 [==============================] - 86s 684ms/step - loss: 0.4193 - accuracy: 0.8074 - val_loss: 0.4259 - val_accuracy: 0.8102\n",
            "Epoch 44/50\n",
            "126/126 [==============================] - 85s 675ms/step - loss: 0.4259 - accuracy: 0.8005 - val_loss: 0.4124 - val_accuracy: 0.8156\n",
            "Epoch 45/50\n",
            "126/126 [==============================] - 85s 673ms/step - loss: 0.4203 - accuracy: 0.8065 - val_loss: 0.4127 - val_accuracy: 0.8181\n",
            "Epoch 46/50\n",
            "126/126 [==============================] - 86s 682ms/step - loss: 0.4262 - accuracy: 0.8032 - val_loss: 0.4584 - val_accuracy: 0.7865\n",
            "Epoch 47/50\n",
            "126/126 [==============================] - 85s 673ms/step - loss: 0.4110 - accuracy: 0.8126 - val_loss: 0.4458 - val_accuracy: 0.8038\n",
            "Epoch 48/50\n",
            "126/126 [==============================] - 85s 674ms/step - loss: 0.4052 - accuracy: 0.8130 - val_loss: 0.4226 - val_accuracy: 0.8033\n",
            "Epoch 49/50\n",
            "126/126 [==============================] - 86s 681ms/step - loss: 0.4044 - accuracy: 0.8167 - val_loss: 0.4130 - val_accuracy: 0.8018\n",
            "Epoch 50/50\n",
            "126/126 [==============================] - 85s 673ms/step - loss: 0.4011 - accuracy: 0.8176 - val_loss: 0.4164 - val_accuracy: 0.8082\n",
            "> 80.821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So training for 50 epochs did improve our model performance. Now lets try transfer learning."
      ],
      "metadata": {
        "id": "Dly-fsdLQ964"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 3 : Transfer Learning"
      ],
      "metadata": {
        "id": "MsjYdqTaRIJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "iaXuwN6JIRlB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vgg16 model used for transfer learning on the dogs and cats dataset\n",
        "import sys\n",
        "from matplotlib import pyplot\n",
        "# from keras.utils import to_categorical\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "# from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# define cnn model\n",
        "def define_model():\n",
        "\t# load model\n",
        "\tmodel_1 = VGG16(include_top=False, input_shape=(224, 224, 3))\n",
        "\t# mark loaded layers as not trainable\n",
        "\tfor layer in model_1.layers:\n",
        "\t\tlayer.trainable = False\n",
        "\t# add new classifier layers\n",
        "\tflat1 = Flatten()(model_1.layers[-1].output)\n",
        "\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
        "\toutput = Dense(1, activation='sigmoid')(class1)\n",
        "\t# define new model\n",
        "\tmodel_1 = Model(inputs=model_1.inputs, outputs=output)\n",
        "\t# compile model\n",
        "\topt = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
        "\tmodel_1.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\treturn model_1\n",
        "\n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Classification Accuracy')\n",
        "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig(filename + '_plot.png')\n",
        "\tpyplot.close()\n",
        "\n",
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "\t# define model\n",
        "\tmodel_1 = define_model()\n",
        "\t# create data generator\n",
        "\tdatagen = ImageDataGenerator(featurewise_center=True)\n",
        "\t# specify imagenet mean values for centering\n",
        "\tdatagen.mean = [123.68, 116.779, 103.939]\n",
        "\t# prepare iterator\n",
        "\ttrain_it = datagen.flow_from_directory('training_set/training_set/',\n",
        "\t\tclass_mode='binary', batch_size=64, target_size=(224, 224))\n",
        "\ttest_it = datagen.flow_from_directory('test_set/test_set/',\n",
        "\t\tclass_mode='binary', batch_size=64, target_size=(224, 224))\n",
        "\t# fit model\n",
        "\thistory_1 = model_1.fit(train_it, steps_per_epoch=len(train_it),\n",
        "\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=10, verbose=1)\n",
        "\t# evaluate model\n",
        "\t_, acc = model_1.evaluate(test_it, steps=len(test_it), verbose=0)\n",
        "\tprint('> %.3f' % (acc * 100.0))\n",
        "\t# learning curves\n",
        "\tsummarize_diagnostics(history_1)\n",
        "\n",
        "# entry point, run the test harness\n",
        "run_test_harness()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFfQ6HUAj7Hf",
        "outputId": "6b7cd93e-dc3d-42f8-f27f-8d29387694a6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8005 images belonging to 2 classes.\n",
            "Found 2023 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 65s 474ms/step - loss: 0.5506 - accuracy: 0.9490 - val_loss: 0.0831 - val_accuracy: 0.9698\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 53s 421ms/step - loss: 0.0257 - accuracy: 0.9909 - val_loss: 0.0867 - val_accuracy: 0.9713\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 53s 416ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.1010 - val_accuracy: 0.9723\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 53s 421ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.1120 - val_accuracy: 0.9718\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 52s 415ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.1226 - val_accuracy: 0.9713\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 52s 415ms/step - loss: 8.5822e-04 - accuracy: 0.9998 - val_loss: 0.1279 - val_accuracy: 0.9723\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 52s 414ms/step - loss: 6.5077e-04 - accuracy: 1.0000 - val_loss: 0.1330 - val_accuracy: 0.9723\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 53s 418ms/step - loss: 5.0933e-04 - accuracy: 1.0000 - val_loss: 0.1368 - val_accuracy: 0.9723\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 52s 413ms/step - loss: 4.2660e-04 - accuracy: 1.0000 - val_loss: 0.1398 - val_accuracy: 0.9723\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 52s 413ms/step - loss: 3.6933e-04 - accuracy: 1.0000 - val_loss: 0.1422 - val_accuracy: 0.9723\n",
            "> 97.232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wooohoooooo 97% accuracy."
      ],
      "metadata": {
        "id": "TXo3_yuiRMcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DMDhYmzAH8V-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}